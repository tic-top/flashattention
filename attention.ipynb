{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash attention in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_attention(Q, K, V, mask=None):\n",
    "    '''\n",
    "    shape of K, Q, V : (seq len, d)\n",
    "    mask: (seq len)\n",
    "    '''\n",
    "    # sacle = 1 / sqrt(d)\n",
    "    scale = Q.size(-1) ** -0.5\n",
    "    scale = 1\n",
    "    Q *= scale\n",
    "    # Q K^T\n",
    "    QK = Q @ K.T\n",
    "    # mask\n",
    "    if mask is not None:\n",
    "        QK.masked_fill_(mask == 0, float('-inf'))\n",
    "    # softmax\n",
    "    QK = QK.softmax(dim=-1)\n",
    "    # QK V\n",
    "    return QK @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512\n",
    "def flash_attention_v1(Q, K, V, mask = None):\n",
    "    '''\n",
    "    forward attention\n",
    "    shape of K, Q, V : (seq len, d)\n",
    "    mask: (seq len)\n",
    "    '''\n",
    "    out = torch.zeros_like(Q)\n",
    "    scale = Q.size(-1) ** -0.5\n",
    "\n",
    "    block_size = min(bs, Q.size(0))\n",
    "\n",
    "    m_prev = -torch.ones(block_size) * float('-inf')\n",
    "    m_curr = -torch.ones(block_size) * float('-inf')\n",
    "    d_prev = torch.zeros(block_size)\n",
    "    d_curr = torch.zeros(block_size)\n",
    "\n",
    "    for j in range(Q.size(0) // block_size):\n",
    "        # Q\n",
    "        d_curr.zero_()\n",
    "        d_prev.zero_()\n",
    "        m_prev.fill_(float('-inf'))\n",
    "        m_curr.fill_(float('-inf'))\n",
    "        Q_block = Q[j*block_size:(j+1)*block_size] * scale\n",
    "        for i in range(Q.size(0) // block_size):\n",
    "            # K, V\n",
    "            K_block = K[i*block_size:(i+1)*block_size]\n",
    "            V_block = V[i*block_size:(i+1)*block_size]\n",
    "            qk = Q_block @ K_block.T # qk: (q_block_size, Kv_block_size)\n",
    "            # get new maximum m from qk\n",
    "            qk_max, _ = torch.max(qk, dim=0)\n",
    "            m_curr = torch.max(m_prev, qk_max)\n",
    "            # update old d\n",
    "            d_prev *= torch.exp(m_prev - m_curr)\n",
    "            p = torch.exp(qk - m_curr[:, None])\n",
    "            d_curr = d_prev + p.sum(dim=1)\n",
    "            # update out\n",
    "            d_inv = 1. / d_curr\n",
    "            p *= d_inv\n",
    "            out[j*block_size:(j+1)*block_size] *= (d_prev * d_inv)[:, None]\n",
    "            out[j*block_size:(j+1)*block_size] += p @ V_block\n",
    "            # update m_prev, d_prev\n",
    "            m_prev = m_curr\n",
    "            d_prev = d_curr\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf time cost:  6.817880868911743\n",
      "flash time cost:  6.2721312046051025\n"
     ]
    }
   ],
   "source": [
    "# record time cost\n",
    "import time\n",
    "hf_time = 0\n",
    "flash_time = 0\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    V = torch.randn(4096, 1024)\n",
    "    t1 = time.time()\n",
    "    A = normal_attention(V, V, V)\n",
    "    hf_time += (time.time() - t1)\n",
    "    t2 = time.time()\n",
    "    B = flash_attention_v1(V, V, V)\n",
    "    flash_time += (time.time() - t2)\n",
    "print('hf time cost: ', hf_time)\n",
    "print('flash time cost: ', flash_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
